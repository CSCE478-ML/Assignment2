{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/winequality-red.csv',sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'quality') #data matrix\n",
    "y = df['quality']  #quality\n",
    "\n",
    "X = np.array(X)  # to np array \n",
    "y = np.array(y)   # to np array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  ## from Assign1\n",
    "best_features = ['volatile acidity', 'density', 'pH', 'sulphates', 'alcohol']\n",
    "best_df = df[best_features]\n",
    "best_df\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(best_df, y, test_size=0.2, random_state=42) ## from assignment 1\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  ## from Assignment 1\n",
    "## 0 mean and unit variance for both X_train and X_test\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import functools \n",
    "\n",
    "def polynomialFeatures(x,degree = 2):\n",
    "        x_t = x.T\n",
    "        features = np.ones(len(x))\n",
    "        for degree in range(1, degree + 1):\n",
    "            for items in itertools.combinations_with_replacement(x_t, degree):\n",
    "                features = np.vstack((features,functools.reduce(lambda x, y: x * y, items)))\n",
    "        return features.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_partition(data,kfold):\n",
    "    return np.array_split(data,kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve(model, X, Y, cv, train_size = 1, learning_rate = 0.01, \n",
    "                   epochs = 1000, tol = None, regularizer = None, lambd = 0.0, **kwargs):\n",
    "    \n",
    "    \n",
    "    if type(train_size) == int and train_size > 1: \n",
    "        train_size_abs = train_size\n",
    "    elif train_size > 0 and train_size <= 1:\n",
    "        train_size_abs = int(train_size * X.shape[0])\n",
    "    else:\n",
    "        print(f'unaceptable train_size of {train_size}')\n",
    "        \n",
    "    if X.shape[0] % train_size_abs != 0:\n",
    "        t = X.shape[0] // train_size_abs + 1\n",
    "    else:\n",
    "        t = X.shape[0] // train_size_abs\n",
    "    \n",
    "    t0 = 1\n",
    "    num_samples_list = []\n",
    "    rmse_training_list = []\n",
    "    rmse_validation_list = []\n",
    "\n",
    "    while t0 <= t:\n",
    "        i = t0 * train_size_abs\n",
    "        if i >= X.shape[0]: i = X.shape[0]\n",
    "        index = np.arange(i)\n",
    "        X_split = (X[index])\n",
    "        Y_split = (Y[index])\n",
    "        \n",
    "        # using the function_2 to calculate the mse, and then rmse using k-fold based on varying training samples.\n",
    "        rmse_tr, rmse_va = np.sqrt(kfold_error(model, X_split, Y_split, cv,\n",
    "                                               learning_rate = learning_rate, epochs = epochs, tol = tol, \n",
    "                                               regularizer = regularizer, lambd = lambd))\n",
    "        num_samples_list.append(i)\n",
    "        rmse_training_list.append(rmse_tr)\n",
    "        rmse_validation_list.append(rmse_va)\n",
    "        t0 += 1\n",
    "\n",
    "    result = {'num_samples':pd.Series(num_samples_list), \n",
    "              'train_scores':pd.Series(rmse_training_list), 'val_score':pd.Series(rmse_validation_list)}\n",
    "    learning_curve_elements = pd.DataFrame(result)\n",
    "    \n",
    "    return np.array(learning_curve_elements[\"train_scores\"]), \\\n",
    "            np.array(learning_curve_elements[\"val_score\"]), np.array(learning_curve_elements[\"num_samples\"]), \\\n",
    "            learning_curve_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_error(model, X, Y, k_fold, learning_rate = 0.01, epochs = 1000, \n",
    "                tol = None, regularizer = None, lambd = 0.0, **kwargs):\n",
    "    if(Y.shape == (Y.shape[0],)):\n",
    "        Y = np.expand_dims(Y,axis=1)\n",
    "    dataset = np.concatenate([X,Y],axis=1)\n",
    "\n",
    "    k_part = k_partition(dataset, k_fold)   # using the function_1 k_partition\n",
    "    \n",
    "    error_training = []\n",
    "    error_validation = []\n",
    "\n",
    "    for idx,val in enumerate(k_part):\n",
    "        validation_Y = val[:,-1]\n",
    "        validation_X = val[:,:-1]\n",
    "        train = np.concatenate(np.delete(k_part,idx,0))\n",
    "        train_Y = train[:,-1]\n",
    "        train_X = train[:,:-1]          \n",
    "    \n",
    "        # with sklearn Linearregression to test the entire function\n",
    "        # replace it when our modeling function is done.\n",
    "        reg = model().fit(train_X, train_Y)\n",
    "        pr_train_Y = reg.predict(train_X)\n",
    "        pr_validation_Y = reg.predict(validation_X)\n",
    "        mse_train_Y = mse(train_Y, pr_train_Y)\n",
    "        mse_validation_Y = mse(validation_Y, pr_validation_Y)\n",
    "    \n",
    "#         # using our modeling function\n",
    "#         model.fit(train_X, train_Y, learning_rate = learning_rate, epochs = epochs, \n",
    "#                   tol = tol, regularizer = regularizer, lambd = lambd)\n",
    "#         pr_train_Y = model.predict(train_X)\n",
    "#         pr_validation_Y = model.predict(validation_X)\n",
    "#         mse_train_Y = mse(train_Y, pr_train_Y)\n",
    "#         mse_validation_Y = mse(validation_Y, pr_validation_Y)\n",
    "\n",
    "        error_training.append(mse_train_Y)\n",
    "        error_validation.append(mse_validation_Y)\n",
    "\n",
    "    # return the average mse for the training and the validation fold.\n",
    "    return np.array(error_training).mean(), np.array(error_validation).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(Y_true, Y_pred):\n",
    "    E = np.array(Y_true).reshape(-1,1) - np.array(Y_pred).reshape(-1,1)\n",
    "    mse = 1/np.array(Y_true).shape[0] * (E.T.dot(E))\n",
    "    return mse[(0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polynomial_model_complexity(model, X, Y, cv, maxPolynomialDegree, \n",
    "                                     learning_rate=0.01, epochs=1000, tol=None, regularizer=None, lambd=0.0, **kwargs):\n",
    "    \n",
    "    poly_list = []\n",
    "    rmse_training_list = []\n",
    "    rmse_validation_list = []\n",
    "    \n",
    "    for i in range(1, maxPolynomialDegree+1):\n",
    "        \n",
    "        # Here should be replaced with our #1 function polynomialFeatures(X, degree)\n",
    "        X_poly = polynomialFeatures(X, i)\n",
    "        Y_poly = polynomialFeatures(X, i)\n",
    "        \n",
    "        rmse_tr, rmse_va = np.sqrt(kfold_error(model, X_poly, Y_poly, cv, \n",
    "                                               learning_rate = learning_rate, epochs = epochs, tol = tol, \n",
    "                                               regularizer = regularizer, lambd = lambd))\n",
    "        poly_list.append(i)\n",
    "        rmse_training_list.append(rmse_tr)\n",
    "        rmse_validation_list.append(rmse_va)\n",
    "        \n",
    "    # plot the RMSE using the list above.\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    \n",
    "    # linewidth and fontsize\n",
    "    lw = 2\n",
    "    fontsize = 20\n",
    "    \n",
    "    # plot curves\n",
    "    plt.plot(poly_list, rmse_training_list, 'o-', color='green', lw = lw, label = \"Training set\") \n",
    "    plt.plot(poly_list, rmse_validation_list, 'o-', color='red', lw = lw, label = \"Validation set\") \n",
    "            \n",
    "    # add title, xlabel, ylabel, and legend. \n",
    "    plt.title('RMSE curve', fontsize = fontsize)\n",
    "    plt.xlabel('Polynomial Degree', fontsize = fontsize)\n",
    "    plt.ylabel('RMSE', fontsize = fontsize)\n",
    "    plt.legend(loc=\"best\", fontsize = fontsize)\n",
    "    plt.xticks(np.arange(1, i+1, 1))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = polynomialFeatures(X_train,degree = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "cv = 5\n",
    "poly = 5\n",
    "#plot_polynomial_model_complexity(LinearRegression, X_train, Y_train, cv, poly, learning_rate = 0.01, \n",
    "#                   epochs = 500, tol = None, regularizer = l2, lambd = 0.0)\n",
    "plot_polynomial_model_complexity(LinearRegression, X_train, y_train, cv, poly, learning_rate = 0.01, \n",
    "                   epochs = 500, tol = None, regularizer = None, lambd = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = polynomialFeatures(X_train,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "a = np.c_[np.ones((a.shape[0],1)),a]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self):\n",
    "        pass   \n",
    "    \n",
    "    def fit(self, X, Y, learning_rate=0.01, epochs=1000, tol=None, regularizer=None,lambd=0.0,**kwargs):\n",
    "        self.degree = 1\n",
    "        if (kwargs != None):\n",
    "            self.degree = kwargs['degree']\n",
    "        self.X = polynomialFeatures(X,self.degree)\n",
    "        epch_counter = 0\n",
    "        m,n = self.X.shape\n",
    "        self.theta = np.zeros(n) # Initialize theta to be 0 of size n\n",
    "        self.error = 1e10\n",
    "        while epch_counter < epochs:\n",
    "            epch_counter += 1 #count the epochs\n",
    "            for i in range(m):\n",
    "                theta = self.theta\n",
    "                error = self.error\n",
    "                a = np.dot(self.X[i].T , (self.X[i] @ self.theta - Y[i]))\n",
    "                if(regularizer == 'l2'):\n",
    "                #j_theta = j_theta + (np.dot(theta[1:].T,theta[1:]) *(lambd /(2*m)))\n",
    "                    self.theta = theta - (a * learning_rate) - (learning_rate * lambd*theta)\n",
    "                \n",
    "                elif(regularizer == 'l1'):\n",
    "                #j_theta = j_theta + (theta[1:].sum() *(lambd /(2*m)))\n",
    "                    self.theta = theta - (a * learning_rate) - (learning_rate * lambd* np.sign(theta))\n",
    "                else:\n",
    "                    self.theta = theta - (a * learning_rate)\n",
    "                if np.linalg.norm(self.theta - theta, ord=1) < tol:\n",
    "                    break\n",
    "        print(f'Epochs needed: {epch_counter}')\n",
    "                                  \n",
    "    def predict(self,X):\n",
    "        X = polynomialFeatures(X,self.degree)\n",
    "        return X @ self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression:\n",
    "    def __init__(self):\n",
    "        pass   \n",
    "    \n",
    "    def fit(self, X, Y, learning_rate=0.01, epochs=1000, tol=None, regularizer=None,lambd=0.0,**kwargs):\n",
    "        self.degree = 1\n",
    "        if (kwargs != None):\n",
    "            self.degree = kwargs['degree']\n",
    "        self.X = polynomialFeatures(X,self.degree)\n",
    "        epch_counter = 0\n",
    "        m,n = self.X.shape\n",
    "        self.theta = np.zeros(n) # Initialize theta to be 0 of size n\n",
    "        self.error = 1e10\n",
    "        self.j_theta = 1e10\n",
    "        while epch_counter < epochs:\n",
    "            epch_counter += 1 #count the epochs\n",
    "            theta = self.theta\n",
    "            error = self.error\n",
    "            j_theta = self.j_theta\n",
    "            self.j_theta = (((Y- self.X @ theta) ** 2).mean()) / 2\n",
    "            print(f'Epoch{epch_counter}: j_theta={self.j_theta}')\n",
    "            a = np.dot(self.X.T,(self.X @ theta - Y)) # gradient of loss wrt parameter\n",
    "            print(f'Epoch{epch_counter}: j_theta={a}')\n",
    "            if(regularizer == 'l2'):\n",
    "                self.j_theta = self.j_theta + (np.dot(theta[1:].T,theta[1:]) *(lambd /(2*m)))\n",
    "                self.theta = theta - ((a * learning_rate) / m) - (learning_rate * lambd*theta)/m \n",
    "                print(f'\\n\\nEpoch{epch_counter}: theta={self.theta}')\n",
    "                \n",
    "            elif(regularizer == 'l1'):\n",
    "                self.j_theta = selfj_theta + (theta[1:].sum() *(lambd /(2*m)))\n",
    "                self.theta = theta - ((a * learning_rate) / m) - (learning_rate * lambd* np.sign(theta))/m\n",
    "            else:\n",
    "                self.theta = theta - ((a * learning_rate) / m)\n",
    "                \n",
    "            #self.error = mean_squared_error(Y, self.X @ self.theta)\n",
    "            #if np.abs(self.error - error) < tol:\n",
    "                #break\n",
    "            if np.abs(self.j_theta - j_theta) < tol:\n",
    "                break\n",
    "        print(f'Epochs needed: {epch_counter}')\n",
    "                                  \n",
    "    def predict(self,X):\n",
    "        X = polynomialFeatures(X,self.degree)\n",
    "        return X @ self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_sgd = Linear_Regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1: j_theta=16.139562157935888\n",
      "Epoch1: j_theta=[-7193.           390.34094757   172.37659605    46.61401311\n",
      "  -250.27039341  -487.62830155 -6985.16937956  -332.07921876\n",
      " -1570.16277466  1905.21705836  1650.45018469 -7436.00527532\n",
      "  2457.18557757  -941.75400489  3673.77015313 -7180.27064955\n",
      "  1437.80749234 -1526.60694007 -7035.30428062  -792.51437935\n",
      " -7554.35093361]\n",
      "\n",
      "\n",
      "Epoch1: theta=[ 0.05623925 -0.00305192 -0.00134775 -0.00036446  0.00195677  0.00381257\n",
      "  0.0546143   0.0025964   0.01227649 -0.01489615 -0.01290422  0.05813921\n",
      " -0.01921177  0.00736321 -0.02872377  0.05613972 -0.01124165  0.01193594\n",
      "  0.05500629  0.00619636  0.05906451]\n",
      "Epoch2: j_theta=14.148466475748146\n",
      "Epoch2: j_theta=[-6712.87942648   413.35288688   102.61778629    23.37523425\n",
      "   -29.91380413  -340.14582557 -6229.72783987  -296.13651422\n",
      " -1353.48766669  1688.01669633  1513.00263442 -6527.61330831\n",
      "  2038.52373097  -748.83479985  3127.57903256 -6211.3561668\n",
      "   868.6363014  -1174.80892297 -5343.57890925  -899.9221793\n",
      " -6728.84269793]\n",
      "\n",
      "\n",
      "Epoch2: theta=[ 0.10872458 -0.00628376 -0.00215007 -0.00054722  0.00219065  0.00647204\n",
      "  0.10332206  0.00491177  0.02285887 -0.02809408 -0.02473379  0.10917602\n",
      " -0.03515018  0.01321805 -0.05317706  0.10470384 -0.01803317  0.0211213\n",
      "  0.0967856   0.01323249  0.11167465]\n",
      "Epochs needed: 2\n"
     ]
    }
   ],
   "source": [
    "lin_reg_sgd.fit(X_train,y_train,learning_rate=0.01,epochs=3,regularizer='l2',lambd= 0.1, tol=10,degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Mean squared error: 25.27\n",
      "Test: Mean squared error: 25.50\n"
     ]
    }
   ],
   "source": [
    "# Make prediction \n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_train_predicted_sgd = lin_reg_sgd.predict(X_train)\n",
    "\n",
    "\n",
    "#lin_reg_sgd.fit(X_test,y_test)\n",
    "y_test_predicted_sgd = lin_reg_sgd.predict(X_test)\n",
    "print(\"Training: Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted_sgd))\n",
    "\n",
    "print(\"Test: Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_test_predicted_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression:\n",
    "    def __init__(self):\n",
    "        pass   \n",
    "    \n",
    "    def fit(self, X, Y, learning_rate=0.01, epochs=1000, tol=None, regularizer=None,lambd=0.0,**kwargs):\n",
    "        epch_counter = 0\n",
    "        m,n = X.shape\n",
    "        self.theta = np.zeros(n) # Initialize theta to be 0 of size n\n",
    "        self.error = -tol\n",
    "        while epch_counter < epochs:\n",
    "            epch_counter += 1 #count the epochs\n",
    "            theta = self.theta\n",
    "            error = self.error\n",
    "            #j_theta = (((Y- X @ theta) ** 2).mean()) / 2\n",
    "            a = np.dot(X.T,(X @ theta - Y)) # gradient of loss wrt parameter\n",
    "            \n",
    "            if(regularizer == 'l2'):\n",
    "                #j_theta = j_theta + (np.dot(theta[1:].T,theta[1:]) *(lambd /(2*m)))\n",
    "                self.theta = theta - ((a * learning_rate) / m) - (learning_rate * lambd*theta)/m\n",
    "                print(self.theta)\n",
    "            elif(regularizer == 'l1'):\n",
    "                #j_theta = j_theta + (theta[1:].sum() *(lambd /(2*m)))\n",
    "                self.theta = theta - ((a * learning_rate) / m) - (learning_rate * lambd* np.sign(theta))/m\n",
    "            else:\n",
    "                self.theta = theta - ((a * learning_rate) / m)\n",
    "            \n",
    "            self.error = mean_squared_error(Y,X @ self.theta)\n",
    "            \n",
    "            if np.abs(self.error - error) < tol:\n",
    "                break\n",
    "\n",
    "        print(f'Epochs needed: {epch_counter}')\n",
    "                                  \n",
    "    def predict(self,X):\n",
    "        return X @ self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 6])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "np.dot(a,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self):\n",
    "        pass   \n",
    "    \n",
    "    def fit(self, X, Y, learning_rate=0.01, epochs=1000, tol=None, regularizer=None,lambd=0.0,**kwargs):\n",
    "        epch_counter = 0\n",
    "        m,n = X.shape\n",
    "        self.theta = np.zeros(n) # Initialize theta to be 0 of size n\n",
    "        self.error = -tol\n",
    "        while epch_counter < epochs:\n",
    "            epch_counter += 1 #count the epochs\n",
    "            for i in range(m):\n",
    "                theta = self.theta\n",
    "                error = self.error\n",
    "                a = np.dot(X[i].T , (X[i] @ self.theta - Y[i]))\n",
    "                if(regularizer == 'l2'):\n",
    "                #j_theta = j_theta + (np.dot(theta[1:].T,theta[1:]) *(lambd /(2*m)))\n",
    "                    self.theta = theta - (a * learning_rate) - (learning_rate * lambd*theta)\n",
    "                    print(self.theta)\n",
    "                \n",
    "                elif(regularizer == 'l1'):\n",
    "                #j_theta = j_theta + (theta[1:].sum() *(lambd /(2*m)))\n",
    "                    self.theta = theta - (a * learning_rate) - (learning_rate * lambd* np.sign(theta))\n",
    "                else:\n",
    "                    self.theta = theta - (a * learning_rate)\n",
    "                #if np.linalg.norm(self.theta - theta, ord=1) < tol:\n",
    "                #    break\n",
    "                self.error = mean_squared_error(Y,X @ self.theta)\n",
    "            \n",
    "                if np.abs(self.error - error) < tol:\n",
    "                    break\n",
    "        print(f'Epochs needed: {epch_counter}')\n",
    "                                  \n",
    "    def predict(self,X):\n",
    "        return X @ self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_sgd = Linear_Regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05623925 -0.00305192 -0.00134775 -0.00036446  0.00195677  0.00381257\n",
      "  0.0546143   0.0025964   0.01227649 -0.01489615 -0.01290422  0.05813921\n",
      " -0.01921177  0.00736321 -0.02872377  0.05613972 -0.01124165  0.01193594\n",
      "  0.05500629  0.00619636  0.05906451]\n",
      "[ 0.06475336 -0.00389759 -0.00109632 -0.00026226  0.00066073  0.00349114\n",
      "  0.06062132  0.00288175  0.01326037 -0.01644737 -0.01464449  0.06371929\n",
      " -0.02012926  0.00746105 -0.03071909  0.06081043 -0.00924377  0.01178907\n",
      "  0.05377838  0.00838781  0.06549447]\n",
      "[ 0.06629049 -0.00410754 -0.00098468 -0.00029181  0.00043513  0.00329679\n",
      "  0.06138481  0.00289713  0.01333416 -0.01666158 -0.014905    0.06430629\n",
      " -0.0200863   0.00743767 -0.03077958  0.06133204 -0.00885247  0.01158802\n",
      "  0.05348083  0.00878536  0.06631855]\n",
      "[ 0.06658592 -0.00415611 -0.00095577 -0.00030436  0.00039931  0.0032423\n",
      "  0.0614842   0.00289421  0.01333454 -0.01669343 -0.01494578  0.0643646\n",
      " -0.02005448  0.00743045 -0.03075397  0.06139566 -0.00878652  0.01153175\n",
      "  0.05344559  0.00885608  0.06643209]\n",
      "[ 0.06664468 -0.00416691 -0.00094937 -0.0003075   0.00039341  0.00322951\n",
      "  0.06149693  0.00289274  0.01333277 -0.0166983  -0.01495233  0.06436925\n",
      " -0.02004563  0.00742874 -0.03074445  0.06140378 -0.00877506  0.01151897\n",
      "  0.05344206  0.00886897  0.06644872]\n",
      "[ 0.06665661 -0.00416926 -0.00094805 -0.00030816  0.00039238  0.00322672\n",
      "  0.06149847  0.0028923   0.01333216 -0.01669906 -0.0149534   0.06436933\n",
      " -0.0200436   0.00742837 -0.03074203  0.06140487 -0.00877299  0.0115163\n",
      "  0.05344178  0.00887137  0.06645131]\n",
      "[ 0.06665907 -0.00416976 -0.00094779 -0.00030829  0.0003922   0.00322614\n",
      "  0.06149863  0.00289219  0.013332   -0.01669917 -0.01495357  0.06436925\n",
      " -0.02004317  0.0074283  -0.03074148  0.06140502 -0.0087726   0.01151577\n",
      "  0.05344177  0.00887183  0.06645173]\n",
      "[ 0.06665958 -0.00416987 -0.00094774 -0.00030831  0.00039216  0.00322602\n",
      "  0.06149864  0.00289217  0.01333197 -0.01669919 -0.0149536   0.06436921\n",
      " -0.02004309  0.00742828 -0.03074136  0.06140504 -0.00877253  0.01151566\n",
      "  0.05344178  0.00887191  0.06645181]\n",
      "Epochs needed: 8\n"
     ]
    }
   ],
   "source": [
    "X_train = polynomialFeatures(X_train,2)\n",
    "X_test = polynomialFeatures(X_test,2)\n",
    "lin_reg_sgd.fit(X_train,y_train,learning_rate=0.01,epochs=500,regularizer='l2',lambd= 1e5, tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Mean squared error: 27.92\n",
      "Test: Mean squared error: 28.35\n"
     ]
    }
   ],
   "source": [
    "# Make prediction \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_train_predicted_sgd = lin_reg_sgd.predict(X_train)\n",
    "\n",
    "\n",
    "#lin_reg_sgd.fit(X_test,y_test)\n",
    "y_test_predicted_sgd = lin_reg_sgd.predict(X_test)\n",
    "print(\"Training: Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted_sgd))\n",
    "\n",
    "print(\"Test: Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_test_predicted_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
