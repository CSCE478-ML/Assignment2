{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  #2 Implementthe following function to calculate and return the mean squared error (mse) of two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_true and Y_pred are column vector, which are n-row × 1-col.\n",
    "\n",
    "def mse(Y_true, Y_pred):\n",
    "    E = np.array(Y_true).reshape(-1,1) - np.array(Y_pred).reshape(-1,1)\n",
    "    mse = 1/np.array(Y_true).shape[0] * (E.T.dot(E))\n",
    "    return mse[(0,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the mse() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.25"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_tr = np.array([1, 3, 5, 6]).reshape(-1,1)\n",
    "Y_pr = np.array([2, 3, 7, 4]).reshape(-1,1)\n",
    "mse(Y_tr, Y_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #3 Implementthe following function to compute training and validation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function_1 used to split the data into k fold\n",
    "\n",
    "def k_partition(data,kfold):\n",
    "    return np.array_split(data,kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function_2 used to compute the mse for each iteration\n",
    "\n",
    "\n",
    "def kfold_error(model, X, Y, k_fold, learning_rate = 0.01, epochs = 1000, \n",
    "                tol = None, regularizer = None, lambd = 0.0, **kwargs):\n",
    "    if(Y.shape == (Y.shape[0],)):\n",
    "        Y = np.expand_dims(Y,axis=1)\n",
    "    dataset = np.concatenate([X,Y],axis=1)\n",
    "\n",
    "    k_part = k_partition(dataset, k_fold)   # using the function_1 k_partition\n",
    "    \n",
    "    error_training = []\n",
    "    error_validation = []\n",
    "\n",
    "    for idx,val in enumerate(k_part):\n",
    "        validation_Y = val[:,-1]\n",
    "        validation_X = val[:,:-1]\n",
    "        train = np.concatenate(np.delete(k_part,idx,0))\n",
    "        train_Y = train[:,-1]\n",
    "        train_X = train[:,:-1]          \n",
    "    \n",
    "        # with sklearn Linearregression to test the entire function\n",
    "        # replace it when our modeling function is done.\n",
    "        reg = model().fit(train_X, train_Y)\n",
    "        pr_train_Y = reg.predict(train_X)\n",
    "        pr_validation_Y = reg.predict(validation_X)\n",
    "        mse_train_Y = mse(train_Y, pr_train_Y)\n",
    "        mse_validation_Y = mse(validation_Y, pr_validation_Y)\n",
    "    \n",
    "#         # using our modeling function\n",
    "#         model.fit(train_X, train_Y, learning_rate = learning_rate, epochs = epochs, \n",
    "#                   tol = tol, regularizer = regularizer, lambd = lambd)\n",
    "#         pr_train_Y = model.predict(train_X)\n",
    "#         pr_validation_Y = model.predict(validation_X)\n",
    "#         mse_train_Y = mse(train_Y, pr_train_Y)\n",
    "#         mse_validation_Y = mse(validation_Y, pr_validation_Y)\n",
    "\n",
    "        error_training.append(mse_train_Y)\n",
    "        error_validation.append(mse_validation_Y)\n",
    "\n",
    "    # return the average mse for the training and the validation fold.\n",
    "    return np.array(error_training).mean(), np.array(error_validation).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final function used to return the training scores and validation scores based on varying training samples. \n",
    "\n",
    "def learning_curve(model, X, Y, cv, train_size = 1, learning_rate = 0.01, \n",
    "                   epochs = 1000, tol = None, regularizer = None, lambd = 0.0, **kwargs):\n",
    "    \n",
    "    \n",
    "    if type(train_size) == int and train_size > 1: \n",
    "        train_size_abs = train_size\n",
    "    elif train_size > 0 and train_size <= 1:\n",
    "        train_size_abs = int(train_size * X.shape[0])\n",
    "    else:\n",
    "        print(f'unaceptable train_size of {train_size}')\n",
    "        \n",
    "    if X.shape[0] % train_size_abs != 0:\n",
    "        t = X.shape[0] // train_size_abs + 1\n",
    "    else:\n",
    "        t = X.shape[0] // train_size_abs\n",
    "    \n",
    "    t0 = 1\n",
    "    num_samples_list = []\n",
    "    rmse_training_list = []\n",
    "    rmse_validation_list = []\n",
    "\n",
    "    while t0 <= t:\n",
    "        i = t0 * train_size_abs\n",
    "        if i >= X.shape[0]: i = X.shape[0]\n",
    "        index = np.arange(i)\n",
    "        X_split = (X[index])\n",
    "        Y_split = (Y[index])\n",
    "        \n",
    "        # using the function_2 to calculate the mse, and then rmse using k-fold based on varying training samples.\n",
    "        rmse_tr, rmse_va = np.sqrt(kfold_error(model, X_split, Y_split, cv, train_size = train_size, \n",
    "                                               learning_rate = learning_rate, epochs = epochs, tol = tol, \n",
    "                                               regularizer = regularizer, lambd = lambd))\n",
    "        num_samples_list.append(i)\n",
    "        rmse_training_list.append(rmse_tr)\n",
    "        rmse_validation_list.append(rmse_va)\n",
    "        t0 += 1\n",
    "\n",
    "    result = {'num_samples':pd.Series(num_samples_list), \n",
    "              'train_scores':pd.Series(rmse_training_list), 'val_score':pd.Series(rmse_validation_list)}\n",
    "    learning_curve_elements = pd.DataFrame(result)\n",
    "    \n",
    "    return np.array(learning_curve_elements[\"train_scores\"]), \\\n",
    "            np.array(learning_curve_elements[\"val_score\"]), np.array(learning_curve_elements[\"num_samples\"]), \\\n",
    "            learning_curve_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the learning_curve function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the lineaRegression from sklearn to test the function. \n",
    "# when our modeling function is done, we need to replace the sklearn function in the function_2\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = 10 * np.random.rand(63).reshape(63,1)\n",
    "Y = 20 * np.random.rand(63).reshape(-1,1)\n",
    "cv = 5\n",
    "train_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.60816592, 5.40296279, 5.76301965, 5.53574231, 5.78068721,\n",
       "        5.76280439, 5.65040963]),\n",
       " array([6.60773878, 6.00045008, 6.31546633, 5.90110402, 6.06903046,\n",
       "        6.0098506 , 5.80802142]),\n",
       " array([10, 20, 30, 40, 50, 60, 63], dtype=int64),\n",
       "    num_samples  train_scores  val_score\n",
       " 0           10      5.608166   6.607739\n",
       " 1           20      5.402963   6.000450\n",
       " 2           30      5.763020   6.315466\n",
       " 3           40      5.535742   5.901104\n",
       " 4           50      5.780687   6.069030\n",
       " 5           60      5.762804   6.009851\n",
       " 6           63      5.650410   5.808021)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_curve(LinearRegression, X, Y, cv, train_size, learning_rate = 0.01, \n",
    "                   epochs = 1000, tol = None, regularizer = None, lambd = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #4 Implementthe following function to plot the training and validation root mean square error (rmse) values of the data matrix X for various polynomial degree starting from 1 up to the value set by “maxPolynomialDegree”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def num_poly_term(d, ploy):\n",
    "    num_ploy_term = np.math.factorial(d + ploy) / (np.math.factorial(d) * np.math.factorial(ploy))\n",
    "    return num_ploy_term\n",
    "\n",
    "def plot_polynomial_model_complexity(model, X, Y, cv, maxPolynomialDegree, \n",
    "                                     learning_rate=0.01, epochs=1000, tol=None, regularizer=None, lambd=0.0, **kwargs)\n",
    "    d = X.shape[1]\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
