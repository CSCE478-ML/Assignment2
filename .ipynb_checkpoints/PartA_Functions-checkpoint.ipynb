{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations_with_replacement\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 polynomialFeatures(X, degree) function\n",
    "\n",
    "def polynomialFeatures(x,degree = 2):\n",
    "    x_t = x.T\n",
    "    features = np.ones(len(x))\n",
    "    for degree in range(1, degree + 1):\n",
    "        for items in combinations_with_replacement(x_t, degree):\n",
    "            features = np.vstack((features,functools.reduce(lambda x, y: x * y, items)))\n",
    "    return features.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 mse(Y_true, Y_pred) function\n",
    "\n",
    "def mse(Y_true, Y_pred):\n",
    "    E = np.array(Y_true).reshape(-1,1) - np.array(Y_pred).reshape(-1,1)\n",
    "    mse = 1/np.array(Y_true).shape[0] * (E.T.dot(E))\n",
    "    return mse[(0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 learning_curve function\n",
    "\n",
    "def k_partition(data,kfold):\n",
    "    return np.array_split(data,kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_error(model, X, Y, k_fold, learning_rate = 0.01, epochs = 1000, \n",
    "                tol = None, regularizer = None, lambd = 0.0, **kwargs):\n",
    "    if(Y.shape == (Y.shape[0],)):\n",
    "        Y = np.expand_dims(Y,axis=1)\n",
    "    dataset = np.concatenate([X,Y],axis=1)\n",
    "\n",
    "    k_part = k_partition(dataset, k_fold)   # using the function_1 k_partition\n",
    "    \n",
    "    error_training = []\n",
    "    error_validation = []\n",
    "\n",
    "    for idx,val in enumerate(k_part):\n",
    "        validation_Y = val[:,-1]\n",
    "        validation_X = val[:,:-1]\n",
    "        train = np.concatenate(np.delete(k_part,idx,0))\n",
    "        train_Y = train[:,-1]\n",
    "        train_X = train[:,:-1]          \n",
    "    \n",
    "        # with sklearn Linearregression to test the entire function\n",
    "        # replace it when our modeling function is done.\n",
    "        reg = model().fit(train_X, train_Y)\n",
    "        pr_train_Y = reg.predict(train_X)\n",
    "        pr_validation_Y = reg.predict(validation_X)\n",
    "        mse_train_Y = mse(train_Y, pr_train_Y)\n",
    "        mse_validation_Y = mse(validation_Y, pr_validation_Y)\n",
    "    \n",
    "#         # using our modeling function\n",
    "#         model.fit(train_X, train_Y, learning_rate = learning_rate, epochs = epochs, \n",
    "#                   tol = tol, regularizer = regularizer, lambd = lambd)\n",
    "#         pr_train_Y = model.predict(train_X)\n",
    "#         pr_validation_Y = model.predict(validation_X)\n",
    "#         mse_train_Y = mse(train_Y, pr_train_Y)\n",
    "#         mse_validation_Y = mse(validation_Y, pr_validation_Y)\n",
    "\n",
    "        error_training.append(mse_train_Y)\n",
    "        error_validation.append(mse_validation_Y)\n",
    "\n",
    "    # return the average mse for the training and the validation fold.\n",
    "    return np.array(error_training).mean(), np.array(error_validation).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve(model, X, Y, cv, train_size = 1, learning_rate = 0.01, \n",
    "                   epochs = 1000, tol = None, regularizer = None, lambd = 0.0, **kwargs):\n",
    "    \n",
    "    \n",
    "    if type(train_size) == int and train_size > 1: \n",
    "        train_size_abs = train_size\n",
    "    elif train_size > 0 and train_size <= 1:\n",
    "        train_size_abs = int(train_size * X.shape[0])\n",
    "    else:\n",
    "        print(f'unaceptable train_size of {train_size}')\n",
    "        \n",
    "    if X.shape[0] % train_size_abs != 0:\n",
    "        t = X.shape[0] // train_size_abs + 1\n",
    "    else:\n",
    "        t = X.shape[0] // train_size_abs\n",
    "    \n",
    "    t0 = 1\n",
    "    num_samples_list = []\n",
    "    rmse_training_list = []\n",
    "    rmse_validation_list = []\n",
    "\n",
    "    while t0 <= t:\n",
    "        i = t0 * train_size_abs\n",
    "        if i >= X.shape[0]: i = X.shape[0]\n",
    "        index = np.arange(i)\n",
    "        X_split = (X[index])\n",
    "        Y_split = (Y[index])\n",
    "        \n",
    "        # using the function_2 to calculate the mse, and then rmse using k-fold based on varying training samples.\n",
    "        rmse_tr, rmse_va = np.sqrt(kfold_error(model, X_split, Y_split, cv, learning_rate = learning_rate, \n",
    "                                               epochs = epochs, tol = tol, regularizer = regularizer, lambd = lambd))\n",
    "        num_samples_list.append(i)\n",
    "        rmse_training_list.append(rmse_tr)\n",
    "        rmse_validation_list.append(rmse_va)\n",
    "        t0 += 1\n",
    "\n",
    "    result = {'num_samples':pd.Series(num_samples_list), \n",
    "              'train_scores':pd.Series(rmse_training_list), 'val_score':pd.Series(rmse_validation_list)}\n",
    "    learning_curve_elements = pd.DataFrame(result)\n",
    "    \n",
    "    return np.array(learning_curve_elements[\"train_scores\"]), \\\n",
    "            np.array(learning_curve_elements[\"val_score\"]), np.array(learning_curve_elements[\"num_samples\"]), \\\n",
    "            learning_curve_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 plot_polynomial_model_complexity function\n",
    "\n",
    "def plot_polynomial_model_complexity(model, X, Y, cv, maxPolynomialDegree, \n",
    "                                     learning_rate=0.01, epochs=1000, tol=None, regularizer=None, lambd=0.0, **kwargs):\n",
    "    \n",
    "    poly_list = []\n",
    "    rmse_training_list = []\n",
    "    rmse_validation_list = []\n",
    "    \n",
    "    for i in range(1, maxPolynomialDegree+1):\n",
    "        \n",
    "        # Here should be replaced with our #1 function polynomialFeatures(X, degree)\n",
    "        X_poly = polynomialFeatures(X, i)\n",
    "        Y_poly = polynomialFeatures(X, i)\n",
    "        \n",
    "        rmse_tr, rmse_va = np.sqrt(kfold_error(model, X_poly, Y_poly, cv, learning_rate = learning_rate, \n",
    "                                               epochs = epochs, tol = tol, regularizer = regularizer, lambd = lambd))\n",
    "        poly_list.append(i)\n",
    "        rmse_training_list.append(rmse_tr)\n",
    "        rmse_validation_list.append(rmse_va)\n",
    "        \n",
    "    # plot the RMSE using the list above.\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    \n",
    "    # linewidth and fontsize\n",
    "    lw = 2\n",
    "    fontsize = 20\n",
    "    \n",
    "    # plot curves\n",
    "    plt.plot(poly_list, rmse_training_list, 'o-', color='green', lw = lw, label = \"Training set\") \n",
    "    plt.plot(poly_list, rmse_validation_list, 'o-', color='red', lw = lw, label = \"Validation set\") \n",
    "            \n",
    "    # add title, xlabel, ylabel, and legend. \n",
    "    plt.title('RMSE curve', fontsize = fontsize)\n",
    "    plt.xlabel('Polynomial Degree', fontsize = fontsize)\n",
    "    plt.ylabel('RMSE', fontsize = fontsize)\n",
    "    plt.legend(loc=\"best\", fontsize = fontsize)\n",
    "    plt.xticks(np.arange(1, i+1, 1))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Linear_Regression\n",
    "\n",
    "class Linear_Regression:\n",
    "    def __init__(self):\n",
    "        pass   \n",
    "    \n",
    "    def fit(self, X, Y, learning_rate=0.01, epochs=1000, tol=None, regularizer=None,lambd=0.0,**kwargs):\n",
    "        epch_counter = 0\n",
    "        m,n = X.shape\n",
    "        print(m, n)\n",
    "        print(Y.shape)\n",
    "        self.theta = np.zeros(n) # Initialize theta to be 0 of size n\n",
    "        print(self.theta)\n",
    "        self.error = -tol\n",
    "        while epch_counter < epochs:\n",
    "            print(epch_counter)\n",
    "            epch_counter += 1 #count the epochs\n",
    "            theta = self.theta\n",
    "            print(\"theta\", theta)\n",
    "            error = self.error\n",
    "            print(\"error\", error)\n",
    "            #j_theta = (((Y- X @ theta) ** 2).mean()) / 2\n",
    "            a = np.dot(X.T,((X @ theta).reshape(-1, 1) - Y)) # gradient of loss wrt parameter\n",
    "            print(\"a:\", a)\n",
    "            if(regularizer == 'l2'):\n",
    "                #j_theta = j_theta + (np.dot(theta[1:].T,theta[1:]) *(lambd /(2*m)))\n",
    "                self.theta = theta - ((a * learning_rate) / m) - (learning_rate * lambd*theta)/m\n",
    "                print(self.theta)\n",
    "            elif(regularizer == 'l1'):\n",
    "                #j_theta = j_theta + (theta[1:].sum() *(lambd /(2*m)))\n",
    "                self.theta = theta - ((a * learning_rate) / m) - (learning_rate * lambd* np.sign(theta))/m\n",
    "            else:\n",
    "                self.theta = theta - ((a * learning_rate) / m)\n",
    "                print(\"theta:\", self.theta)\n",
    "                print(self.theta.shape)\n",
    "                \n",
    "            print(\"X:\", X)\n",
    "            print(X.shape)\n",
    "            \n",
    "            self.error = mse(Y,X @ self.theta)\n",
    "            print(self.error)\n",
    "            \n",
    "            if np.abs(self.error - error) < tol:\n",
    "                break\n",
    "\n",
    "        print(f'Epochs needed: {epch_counter}')\n",
    "                                  \n",
    "    def predict(self,X):\n",
    "        return X @ self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self):\n",
    "        pass   \n",
    "    \n",
    "    def fit(self, X, Y, learning_rate=0.01, epochs=1000, tol=None, regularizer=None,lambd=0.0,**kwargs):\n",
    "        epch_counter = 0\n",
    "        m,n = X.shape\n",
    "        self.theta = np.zeros(n) # Initialize theta to be 0 of size n\n",
    "        self.error = -tol\n",
    "        while epch_counter < epochs:\n",
    "            epch_counter += 1 #count the epochs\n",
    "            for i in range(m):\n",
    "                theta = self.theta\n",
    "                error = self.error\n",
    "                a = np.dot(X[i].T , (X[i] @ self.theta - Y[i]))\n",
    "                if(regularizer == 'l2'):\n",
    "                #j_theta = j_theta + (np.dot(theta[1:].T,theta[1:]) *(lambd /(2*m)))\n",
    "                    self.theta = theta - (a * learning_rate) - (learning_rate * lambd*theta)\n",
    "                    print(self.theta)\n",
    "                \n",
    "                elif(regularizer == 'l1'):\n",
    "                #j_theta = j_theta + (theta[1:].sum() *(lambd /(2*m)))\n",
    "                    self.theta = theta - (a * learning_rate) - (learning_rate * lambd* np.sign(theta))\n",
    "                else:\n",
    "                    self.theta = theta - (a * learning_rate)\n",
    "                #if np.linalg.norm(self.theta - theta, ord=1) < tol:\n",
    "                #    break\n",
    "                self.error = mse(Y,X @ self.theta)\n",
    "            \n",
    "                if np.abs(self.error - error) < tol:\n",
    "                    break\n",
    "        print(f'Epochs needed: {epch_counter}')\n",
    "                                  \n",
    "    def predict(self,X):\n",
    "        return X @ self.theta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
